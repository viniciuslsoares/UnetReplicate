{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "import logging\n",
    "import torch\n",
    "from EfficientUnet import EfficientUnet\n",
    "import importlib\n",
    "from datasets import BasicDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap = ListedColormap([[0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "                        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "                        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "                        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "                        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "                        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353]]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cuda\n",
      "INFO: Network:\n",
      "\t2 input channels\n",
      "\t6 output channels (classes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f'Using device {device}')\n",
    "model = EfficientUnet(n_classes=6).to(device)\n",
    "\n",
    "logging.info(f'Network:\\n'\n",
    "        f'\\t{model.n_channels} input channels\\n'\n",
    "        f'\\t{model.n_classes} output channels (classes)\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/images/train'\n",
    "train_mask_dir = 'data/annotations/train'\n",
    "val_dir = 'data/images/val'\n",
    "val_mask_dir = 'data/annotations/val'\n",
    "test_dir = 'data/images/test'\n",
    "test_mask_dir = 'data/annotations/test'\n",
    "checkpoint_dir = 'checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Creating dataset with 400 examples\n",
      "INFO: Scanning mask files to determine unique values\n",
      "100%|██████████| 400/400 [00:00<00:00, 3642.41it/s]\n",
      "INFO: Unique mask values: [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = BasicDataset(test_dir, test_mask_dir, augmentation=False)\n",
    "# tese_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[701, 401]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (701 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m EfficientUnet(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdir_img_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdir_mask_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_mask_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdir_img_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdir_mask_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_mask_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdir_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_clipping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/UnetReplicate/train.py:55\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, device, dir_img_train, dir_mask_train, dir_img_val, dir_mask_val, dir_checkpoint, save_checkpoint, epochs, batch_size, learning_rate, val_percent, augmentation, gradient_clipping, k_folds)\u001b[0m\n\u001b[1;32m     53\u001b[0m val_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m splits_sizes:\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mval_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# for i, j in enumerate()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(val_ids)\n",
      "\u001b[0;31mTypeError\u001b[0m: list.append() takes exactly one argument (701 given)"
     ]
    }
   ],
   "source": [
    "importlib.reload(train)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10\n",
    "model = EfficientUnet(n_classes=6).to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train.train_model(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    dir_img_train=train_dir,\n",
    "    dir_mask_train=train_mask_dir,\n",
    "    dir_img_val=val_dir,\n",
    "    dir_mask_val=val_mask_dir,\n",
    "    dir_checkpoint=checkpoint_dir,\n",
    "    save_checkpoint=True,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    val_percent=0.1,\n",
    "    augmentation=True,\n",
    "    gradient_clipping=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(test_dataset))\n",
    "\n",
    "instancia = test_dataset[idx]\n",
    "\n",
    "data_teste, mask_teste = instancia['image'], instancia['mask']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data_teste = data_teste.unsqueeze(0).to(device).float()\n",
    "    mask_pred = model(data_teste)\n",
    "    mask_pred = mask_pred.squeeze(0).cpu()\n",
    "    mask_pred = torch.argmax(mask_pred, dim=0)\n",
    "\n",
    "axs[0].imshow(mask_teste, cmap=cmap)\n",
    "axs[1].imshow(mask_pred, cmap=cmap)\n",
    "\n",
    "# Remove the axis labels\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassJaccardIndex, MulticlassF1Score\n",
    "\n",
    "IoU = MulticlassJaccardIndex(num_classes=model.n_classes).to(device=device)\n",
    "F1_score = MulticlassF1Score(num_classes=model.n_classes).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_input_data, load_image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "cp_num = EPOCHS\n",
    "# model.load_state_dict(torch.load(f'checkpoints/checkpoint{cp_num}.pth', map_location=device))\n",
    "# model_teste = EfficientUnet(n_classes=6).to(device)\n",
    "# model_teste.load_state_dict(torch.load(f'checkpoints/checkpoint{cp_num}.pth', map_location=device))\n",
    "\n",
    "il_list = [32, 109]\n",
    "masks = []\n",
    "preds = []\n",
    "lista = []\n",
    "fig, axs = plt.subplots(len(il_list), 2, figsize=(12, 4))\n",
    "\n",
    "for i, il in enumerate(il_list):\n",
    "    data_path = 'data/images/test/il_{}.tif'.format(il)\n",
    "    mask_path = 'data/annotations/test/il_{}.png'.format(il)\n",
    "    \n",
    "    data_teste = get_input_data(data_path)\n",
    "    mask_teste = load_image(mask_path)\n",
    "    data_teste = torch.tensor(data_teste).unsqueeze(0).to(device).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    mask_pred = model(data_teste)\n",
    "    mask_pred = mask_pred.squeeze(0).cpu()\n",
    "    mask_pred = torch.argmax(mask_pred, dim=0)[:mask_teste.shape[0], :mask_teste.shape[1]]\n",
    "    print(data_teste.shape, mask_teste.shape, mask_pred.shape)\n",
    "    \n",
    "    difference = np.where(mask_teste == np.array(mask_pred), 1, 0)    \n",
    "    \n",
    "    axs[0, i].imshow(mask_teste, cmap=cmap)\n",
    "    axs[1, i].imshow(mask_pred, cmap=cmap)\n",
    "    # axs[i, 1].set_title('Inline {}'.format(il))\n",
    "    # axs[i, 2].imshow(difference, cmap='gray')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for ax in axs[i]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    lista.append(difference)\n",
    "    masks.append(mask_teste)\n",
    "    preds.append(mask_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].imshow(lista[0], cmap='grey')\n",
    "axs[1].imshow(lista[1], cmap='grey')\n",
    "axs[0].axis('off')\n",
    "axs[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(preds):\n",
    "    iou = IoU(img.unsqueeze(0).to(device), torch.tensor(masks[i]).unsqueeze(0).to(device))\n",
    "    f1 = F1_score(img.unsqueeze(0).to(device), torch.tensor(masks[i]).unsqueeze(0).to(device))\n",
    "    print(f'Inline {il_list[i]} -> IoU: {iou:.2f}; F1: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = BasicDataset(test_dir, test_mask_dir, augmentation=False)\n",
    "teste_dl = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_model\n",
    "\n",
    "iou, f1, loss = evaluate_model(model, teste_dl, device, window=False)\n",
    "print(f'IoU: {iou:.2f}; F1: {f1:.2f}; Loss: {loss:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
